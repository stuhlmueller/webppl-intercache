x Improvements to the system (Andreas)
  x Turn into webppl package
  x Don't resample/use factors when you predict (to avoid seemingly low variance due to particle collapse)
  x Use single score/factor instead of many

x MH will add RSA model that illustrates inferring lambda
x MH will plot The Big Sum -- p(d|m) -- as a function of lambda

x Make figures that illustrate performance on slowBinomial (Jason)
  x error as a function of time (different curves for different parameter settings)
  x to explore space of parameters, could do grid search or random sampling

x Andreas: figure out why current system doesn't learn
  x would using SampleGuide instead of SMC fix the problem?
    do this if disableConditioning is on (rename)

x Send a progress report to Noah that describes what we have done
  so far (and what we will do next); maybe send to MH and Andreas first

x make better visualizations for current simple model
  x think about what figure we want
  x send proposal for figure (w/o actually making it) to mh and andreas
  x then gather data (if needed) and make figure

x abstract out prior on functions


x Next meeting: Fri 1pm

x Andreas: factor out mutation to library.js file

x for running time per iteration plot, also make version of plots with window size 10
  (to better understand initial overhead)

x let polynomial degree be an option

x (augment data collection)

x MH: make model slower
x MH: look into (uninformative) priors on regression variance
x also visualize raw data (input-output pairs observed) + guide predictions


- generatePredictions for non-input values as well
- test with degree 3 and 4 polynomial distOnFuncs on (linear) binomial model, visualize
- geom_smooth: how to get actual 95% confidence interval, or something like that?

- look at predictions when we only have few data points from slowFunc

- test with binomial variance (or some other non-linear function)

- verify that, if family of approximating functions is too limited (e.g. f(x)=1, f(x)=c), we don't converge to using the cache
  - if we do converge to using the cache, think about how to fix it

- test with polynomial distOnFuncs on mh's data
  1. mh: fix prevalence and prior so that speaker2 is slowFunc of 1 argument
  2. apply intercache to speaker2 with prevalence and prior fixed
  3. generalize our cache to multiple arguments
  4. try with slowFunc of multiple arguments (prevalence, prior, speakerOptimality)


- figure out neural net function prior (andreas)
- test with neural net distOnFuncs on binomial model
- test with neural net distOnFuncs on mh's data


- (try on spatial language model)

- (look at gpcache notes)

--------------------------------------------------------------------


- given parameters that work well, do we actually see an improvement in speed over slowBinomial?
  What is the tradeoff between speed gain and loss in accuracy?

- (optimize variance of guide distribution as well?)

- Then we'll think about approximating families that could deal with this
  - e.g. try simple Bayesian neural nets
    - deterministic weights until final layer (adaptive basis function regression)
    - writing full bayesian neural net may be simpler?
  - e.g. wavelet approximation

- We'll have to do some profiling / think about how to make it fast
  - first, figure out what's slow
  - e.g. don't always condition on all data when doing parameter optimization -
    randomly (?) select fixed-size subset
  - e.g. replace mean and variance with fast js versions
  - e.g. use foreach2


Later:
- explore maximum likelihood version (without prior)
- generalize to multiple arguments; different function approximators
- share variance between polynomial coefficients
