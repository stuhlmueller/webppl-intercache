x Improvements to the system (Andreas)
  x Turn into webppl package
  x Don't resample/use factors when you predict (to avoid seemingly low variance due to particle collapse)
  x Use single score/factor instead of many

x MH will add RSA model that illustrates inferring lambda
x MH will plot The Big Sum -- p(d|m) -- as a function of lambda


x Make figures that illustrate performance on slowBinomial (Jason)
  x error as a function of time (different curves for different parameter settings)
  x to explore space of parameters, could do grid search or random sampling

x Andreas: figure out why current system doesn't learn
  x would using SampleGuide instead of SMC fix the problem?
    do this if disableConditioning is on (rename)

x Send a progress report to Noah that describes what we have done
  so far (and what we will do next); maybe send to MH and Andreas first

x make better visualizations for current simple model
  x think about what figure we want
  x send proposal for figure (w/o actually making it) to mh and andreas
  x then gather data (if needed) and make figure

x abstract out prior on functions


- Next meeting: Fri 1pm

- Andreas: factor out mutation to library.js file

x for running time per iteration plot, also make version of plots with window size 10
  (to better understand initial overhead)

- test with polynomial distOnFuncs on (linear) binomial model
  - let polynomial degree be an option
  - also visualize raw data (input-output pairs observed) + best function learned
    - (augment data collection)
    
- test with binomial variance (or some other non-linear function)
- test with polynomial distOnFuncs on mh's data

- MH: look into (uninformative) priors on regression variance

- MH: make model slower

- test with neural net distOnFuncs on binomial model (andreas)
- test with neural net distOnFuncs on mh's data


- (try on spatial language model)

- (look at gpcache notes)

--------------------------------------------------------------------


- given parameters that work well, do we actually see an improvement in speed over slowBinomial?
  What is the tradeoff between speed gain and loss in accuracy?

- (optimize variance of guide distribution as well?)

- Then we'll think about approximating families that could deal with this
  - e.g. try simple Bayesian neural nets
    - deterministic weights until final layer (adaptive basis function regression)
    - writing full bayesian neural net may be simpler?
  - e.g. wavelet approximation

- We'll have to do some profiling / think about how to make it fast
  - first, figure out what's slow
  - e.g. don't always condition on all data when doing parameter optimization -
    randomly (?) select fixed-size subset
  - e.g. replace mean and variance with fast js versions
  - e.g. use foreach2


Later:
- explore maximum likelihood version (without prior)
- generalize to multiple arguments; different function approximators
- share variance between polynomial coefficients
