- try high-degree polynomial as estimator for quadratic function, but only provide training data for part of space
  - only supply data for smaller region; use high-degree polynomial
  - we want to see hit rate vs. iteration vs. input-val (or, discretized into region)
    - this could be done: (a) facet by width (small, medium, large)
    - MH WILL DO (b) facet by iteration-epoch (0-100, 100-200, 200-300,...)
      (given data and infrastructure for (a) exists)
  - do this independent of data restriction experiment below
  -> check whether variance in predictions is higher for unseen part of space

- restrict how much data to learn from so that runtime doesn't slow down linearly (select a random subset of previous data? always include current data point)
  - more efficiently select subset: don't zip data first, get subset directly, maybe by sampling indices first
    - secret randomness: Math.floor(util.random() * 10);

- continue learning with some probability even if predicted variance is low (so that we converge to correct function eventually if it's past of our approximating functions)

- 1pm Monday meeting

- test with polynomial distOnFuncs on mh's data
  x mh: fix prevalence and prior so that speaker2 is slowFunc of 1 argument
  - apply intercache to speaker2 with prevalence and prior fixed
  - (generalize our cache to multiple arguments)
  - (try with slowFunc of multiple arguments (prevalence, prior, speakerOptimality))


- would running more or bigger gradient steps give closer approximations to linear functions for polynomials?

- what do the learned final distributions on slope (mean + variance), offset etc. look like?



x figure out neural net function prior (andreas)
  - make it work with wider range of input values (of different magnitudes)
    - try with MH's data in tests/generics/results
  - integrate it with intercache system
  
- test with neural net distOnFuncs on binomial model
- test with neural net distOnFuncs on mh's data

--------------------------------------------------------------------

- (try on spatial language model)

- (look at gpcache notes)

- given parameters that work well, do we actually see an improvement in speed over slowBinomial?
  What is the tradeoff between speed gain and loss in accuracy?

- (optimize variance of guide distribution as well?)

- Then we'll think about approximating families that could deal with this
  - e.g. try simple Bayesian neural nets
    - deterministic weights until final layer (adaptive basis function regression)
    - writing full bayesian neural net may be simpler?
  - e.g. wavelet approximation

- We'll have to do some profiling / think about how to make it fast
  - first, figure out what's slow
  - e.g. don't always condition on all data when doing parameter optimization -
    randomly (?) select fixed-size subset
  - e.g. replace mean and variance with fast js versions
  - e.g. use foreach2


Later:
- explore maximum likelihood version (without prior)
- generalize to multiple arguments; different function approximators
- share variance between polynomial coefficients
