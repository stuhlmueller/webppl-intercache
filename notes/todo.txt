- summarize the current state of things for meeting with noah, including what we've done so far (look at done.txt file as well)

- try high-degree polynomial as estimator for quadratic function, but only provide training data for part of space
  - only supply data for smaller region; use high-degree polynomial
  -> variance is not higher for unseen part of space
  -> we suspect it's due to variational inference approximation
  -> try using hmc inference instead of variational inference
     - adjust constants:
       x explore lower regression variance
       x higher threshold
     x do measurement not based on actually observed input values, but rather in post-pass ("predictive samples"; use 100 samples)
       - use 1000 samples?
         try to understand why predicted variance doesn't look smoother
     x increase number of samples for hmc / or adjust hmc parameters otherwise
       (empirical variance seems to be all over the map currently; noisy estimates?)
       x let's use 1000 samples, 200 burn-in
     x do inference on regression variance (introduce regressionVarianceMean parameter)
       x regression variance is sampled
       x use gamma distribution?

- continue learning with some probability even if predicted variance is low (so that we converge to correct function eventually if it's part of our approximating functions)
  - see if this works with data restriction (above)
  -> ELBO scoreDiff not finite error is still happening
     x try gd and adam instead of adagrad -> no improvement
     - need to debug
       - (maybe we can come up with a minimal example in the future)
       - advance daipp branch, see if it still happens
       - create a script that reproduces the error (maybe just our repo at a specific commit); write a short description; then we'll send it to paul
         x can open an issue on intercache
  - show that we converge perfectly to the correct function eventually (e.g. if true func is degree-2 polynomial, and approximating family is as well)

- maybe play with demo.wppl some more, see if behavior is surprising in any way

--------------------------------------------------------------------

- test with polynomial distOnFuncs on mh's data
  x mh: fix prevalence and prior so that speaker2 is slowFunc of 1 argument
  - apply intercache to speaker2 with prevalence and prior fixed
  - (generalize our cache to multiple arguments)
  - (try with slowFunc of multiple arguments (prevalence, prior, speakerOptimality))


- would running more or bigger gradient steps give closer approximations to linear functions for polynomials?

- what do the learned final distributions on slope (mean + variance), offset etc. look like?



x figure out neural net function prior (andreas)
  - make it work with wider range of input values (of different magnitudes)
    - try with MH's data in tests/generics/results
  - integrate it with intercache system
  
- test with neural net distOnFuncs on binomial model
- test with neural net distOnFuncs on mh's data

--------------------------------------------------------------------

- (try on spatial language model)

- (look at gpcache notes)

- given parameters that work well, do we actually see an improvement in speed over slowBinomial?
  What is the tradeoff between speed gain and loss in accuracy?

- (optimize variance of guide distribution as well?)

- Then we'll think about approximating families that could deal with this
  - e.g. try simple Bayesian neural nets
    - deterministic weights until final layer (adaptive basis function regression)
    - writing full bayesian neural net may be simpler?
  - e.g. wavelet approximation

- We'll have to do some profiling / think about how to make it fast
  - first, figure out what's slow
  - e.g. don't always condition on all data when doing parameter optimization -
    randomly (?) select fixed-size subset
  - e.g. replace mean and variance with fast js versions
  - e.g. use foreach2


Later:
- explore maximum likelihood version (without prior)
- generalize to multiple arguments; different function approximators
- share variance between polynomial coefficients
