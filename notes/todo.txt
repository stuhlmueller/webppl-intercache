x Improvements to the system (Andreas)
  x Turn into webppl package
  x Don't resample/use factors when you predict (to avoid seemingly low variance due to particle collapse)
  x Use single score/factor instead of many

x MH will add RSA model that illustrates inferring lambda
x MH will plot The Big Sum -- p(d|m) -- as a function of lambda

- Make figures that illustrate performance on slowBinomial (Jason)
  - error as a function of time (different curves for different parameter settings)
  - to explore space of parameters, could do grid search or random sampling
  - given parameters that work well, do we actually see an improvement in speed over slowBinomial?
    What is the tradeoff between speed gain and loss in accuracy?


- Then we'll think about approximating families that could deal with this
  - e.g. try simple Bayesian neural nets
    - deterministic weights until final layer (adaptive basis function regression)
    - writing full bayesian neural net may be simpler?
  - e.g. wavelet approximation

- We'll have to do some profiling / think about how to make it fast
  - first, figure out what's slow
  - e.g. don't always condition on all data when doing parameter optimization -
    randomly (?) select fixed-size subset
  - e.g. replace mean and variance with fast js versions
  - e.g. use foreach2


Later:
- explore maximum likelihood version (without prior)
- generalize to multiple arguments; different function approximators
- optimize variance of guide distribution as well?
- measuring how well we're doing
