[x] merge restriction on number of data points

- try high-degree polynomial as estimator for quadratic function, but only provide training data for part of space
  - only supply data for smaller region; use high-degree polynomial
  -> variance is not higher for unseen part of space
  -> we suspect it's due to variational inference approximation
  -> try using hmc inference instead of variational inference
     - adjust constants:
       - explore lower regression variance
       - higher threshold
     - increase number of samples for hmc / or adjust hmc parameters otherwise
       (empirical variance seems to be all over the map currently; noisy estimates?)
     - do inference on regression variance (introduce regressionVarianceMean parameter)

- continue learning with some probability even if predicted variance is low (so that we converge to correct function eventually if it's past of our approximating functions)
  - see if this works with data restriction (above)
  -> ELBO scoreDiff not finite error is still happening
     - try gd and adam instead of adagrad
     - need to debug
       - maybe we can come up with a minimal example in the future

Fri 11am pacific / 2pm east coast time; skype + screenshare

--------------------------------------------------------------------

- test with polynomial distOnFuncs on mh's data
  x mh: fix prevalence and prior so that speaker2 is slowFunc of 1 argument
  - apply intercache to speaker2 with prevalence and prior fixed
  - (generalize our cache to multiple arguments)
  - (try with slowFunc of multiple arguments (prevalence, prior, speakerOptimality))


- would running more or bigger gradient steps give closer approximations to linear functions for polynomials?

- what do the learned final distributions on slope (mean + variance), offset etc. look like?



x figure out neural net function prior (andreas)
  - make it work with wider range of input values (of different magnitudes)
    - try with MH's data in tests/generics/results
  - integrate it with intercache system
  
- test with neural net distOnFuncs on binomial model
- test with neural net distOnFuncs on mh's data

--------------------------------------------------------------------

- (try on spatial language model)

- (look at gpcache notes)

- given parameters that work well, do we actually see an improvement in speed over slowBinomial?
  What is the tradeoff between speed gain and loss in accuracy?

- (optimize variance of guide distribution as well?)

- Then we'll think about approximating families that could deal with this
  - e.g. try simple Bayesian neural nets
    - deterministic weights until final layer (adaptive basis function regression)
    - writing full bayesian neural net may be simpler?
  - e.g. wavelet approximation

- We'll have to do some profiling / think about how to make it fast
  - first, figure out what's slow
  - e.g. don't always condition on all data when doing parameter optimization -
    randomly (?) select fixed-size subset
  - e.g. replace mean and variance with fast js versions
  - e.g. use foreach2


Later:
- explore maximum likelihood version (without prior)
- generalize to multiple arguments; different function approximators
- share variance between polynomial coefficients
