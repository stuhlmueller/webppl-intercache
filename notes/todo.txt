- try high-degree polynomial as estimator for quadratic function, but only provide training data for part of space
  - this may have been successful.
  - we want to see hit rate vs. iteration vs. input-val (or, discretized into region)
    - this could be done: (a) facet by region ([0, 0.25], ..., [0.75,1.0])
    - or (b) facet by iteration-epoch (0-100, 100-200, 200-300,...)
  - to confirm: try this with different regions e.g. [0.75, 1], [0,0.25], [0.4,0.6]
  - do this independent of data restriction experiment below
  -> check whether variance in predictions is higher for unseen part of space
  - this will probably fail
  - (then we'll need to look into making variational guide for polynomial parameters less independent (not mean-field))

- restrict how much data to learn from so that runtime doesn't slow down linearly (select a random subset of previous data? always include current data point)
  - this didn't work.
  - this may be because the selectN was implemented inefficiently. need to try _.sample(data, n), while enforcing keeping last data-point.

- continue learning with some probability even if predicted variance is low (so that we converge to correct function eventually if it's past of our approximating functions)

- test with polynomial distOnFuncs on mh's data
  x mh: fix prevalence and prior so that speaker2 is slowFunc of 1 argument
  - apply intercache to speaker2 with prevalence and prior fixed
  - (generalize our cache to multiple arguments)
  - (try with slowFunc of multiple arguments (prevalence, prior, speakerOptimality))

- would running more or bigger gradient steps give closer approximations to linear functions for polynomials?

- what do the learned final distributions on slope (mean + variance), offset etc. look like?



x figure out neural net function prior (andreas)
  - make it work with wider range of input values (of different magnitudes)
    - try with MH's data in tests/generics/results
  - integrate it with intercache system
  
- test with neural net distOnFuncs on binomial model
- test with neural net distOnFuncs on mh's data

--------------------------------------------------------------------

- (try on spatial language model)

- (look at gpcache notes)

- given parameters that work well, do we actually see an improvement in speed over slowBinomial?
  What is the tradeoff between speed gain and loss in accuracy?

- (optimize variance of guide distribution as well?)

- Then we'll think about approximating families that could deal with this
  - e.g. try simple Bayesian neural nets
    - deterministic weights until final layer (adaptive basis function regression)
    - writing full bayesian neural net may be simpler?
  - e.g. wavelet approximation

- We'll have to do some profiling / think about how to make it fast
  - first, figure out what's slow
  - e.g. don't always condition on all data when doing parameter optimization -
    randomly (?) select fixed-size subset
  - e.g. replace mean and variance with fast js versions
  - e.g. use foreach2


Later:
- explore maximum likelihood version (without prior)
- generalize to multiple arguments; different function approximators
- share variance between polynomial coefficients
